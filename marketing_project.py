# -*- coding: utf-8 -*-
"""Αντίγραφο Αντίγραφο project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fwGpAnxcIZHcx85wvidlSelWvj0C9xR7
"""

!pip install feature-engine
!pip install category_encoders
!pip install catboost


# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import category_encoders as ce
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from scipy.stats import boxcox, yeojohnson
from scipy.special import inv_boxcox
from category_encoders import OrdinalEncoder, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report, make_scorer, f1_score
from google.colab import files
from sklearn.preprocessing import MinMaxScaler

# Load dataset
url = "marketing_campaign.csv"
df = pd.read_csv(url)

# print 5 first observations to start getting familiar with the dataset
df.head()

# print column types
df.dtypes

# Missing values
missing_values = df.isnull().sum()
print("Missing Values per Column:")
missing_values
# Βήμα 1: Εύρεση του πλήθους και του ποσοστού των missing τιμών
missing_count = df['Income'].isnull().sum()
total_count = len(df)
missing_percentage = (missing_count / total_count) * 100

print(f"Number of missing values: {missing_count}")
print(f"Percentage of missing values: {missing_percentage:.2f}%")

import numpy as np

# Συνάρτηση που βρίσκει τα outliers μιας pandas Series
def find_outliers(series):
    # Αφαιρούμε NaN για τον υπολογισμό
    s = series.dropna()
    q1 = s.quantile(0.25)
    q3 = s.quantile(0.75)
    iqr = q3 - q1
    lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr
    return s[(s < lower) | (s > upper)]

# 1. Βρες τις αριθμητικές στήλες στο train set
num_features = df.select_dtypes(include=['int64', 'float64']).columns

# 2. Εντοπισμός outliers ανά στήλη
outlier_info = {}
for col in num_features:
    out = find_outliers(df[col])
    if not out.empty:
        outlier_info[col] = out

# 3. Εκτύπωση αποτελεσμάτων
if not outlier_info:
    print("Δεν βρέθηκαν outliers σε καμία αριθμητική στήλη.")
else:
    for col, out in outlier_info.items():
        print(f"Στήλη: {col}")
        print(f"  • Outliers: {len(out)} τιμές")
        print(f"  • Τιμές: {np.unique(out.values)}\n")

# Summary statistics for numerical features
print("Summary Statistics:")
df.describe()

print(df.describe().loc[['min', 'max']])



# 1. Φιλτράρεις μόνο τις αριθμητικές στήλες
num_cols = df.select_dtypes(include=[np.number]).columns
# num_corr = df.corr(numeric_only=True)

# 2. Υπολογίζεις τον πίνακα συσχέτισης μόνο για αυτές
corr_matrix = df[num_cols].corr(method='pearson')

# 3. Σχεδιάζεις το heatmap
fig, ax = plt.subplots(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)
ax.set_title("Correlation Matrix (float & integer features)")
plt.show()

df['Response'].value_counts().plot(kind='bar')
#unbalanced

df[['Year_Birth','Recency']].boxplot()
plt.show()

df[['Income']].boxplot()
plt.show()

# Βρίσκει τις στήλες που έχουν ακριβώς μία μοναδική τιμή
constant_columns = [col for col in df.columns if df[col].nunique() == 1]
print("Οι στήλες με την ίδια τιμή σε κάθε γραμμή είναι:", constant_columns)

df.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True)

print(df['Year_Birth'].describe())

# Θέτουμε το όριο (threshold) ως 1902
threshold = 1902

# Φιλτράρουμε το DataFrame ώστε να κρατήσουμε μόνο τις γραμμές όπου το Year_Birth >= threshold
df = df[df['Year_Birth'] >= threshold]

# Έλεγχος αποτελεσμάτων: εμφάνιση του summary της στήλης Year_Birth
print(df['Year_Birth'].describe())

print(df['Income'].describe())

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(df['Income'].dropna(), kde=True)
plt.title('histogram of income')
plt.xlabel('values')
plt.ylabel('frequency')

#df = df[df['Income'] != 666666.000000]
#print(df['Income'].describe())

plt.subplot(1, 2, 2)
sns.boxplot(x=df['Income'])
plt.title('Boxplot of income')

plt.tight_layout()
plt.show()

# Get the unique values in the Education column
unique_values = df['Education'].unique()

# Get the number of unique values
num_unique = df['Education'].nunique()

print("Number of unique values in Education column:", num_unique)
print("Unique values:", unique_values)

print("\nValue counts:")
print(df['Marital_Status'].value_counts())

# Ορισμός των σπάνιων τιμών που θέλουμε να συγχωνεύσουμε
rare_statuses = ["Alone", "Absurd", "YOLO"]

# Αντικατάσταση των τιμών στη στήλη "Marital_Status" με "Else"
df["Marital_Status"] = df["Marital_Status"].replace(rare_statuses, "Else")
# Έλεγχος των value counts μετά την αντικατάσταση
print(df["Marital_Status"].value_counts())

# Υποθέτουμε ότι το DataFrame σου ονομάζεται df και η στήλη Dt_Customer έχει τις ημερομηνίες σε μορφή 'dd/mm/yyyy'
# Μετατροπή της στήλης σε datetime
df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d/%m/%Y')

# Ομαδοποίηση των εγγραφών ανά μήνα και υπολογισμός του αριθμού εγγραφών
registrations = df.groupby(pd.Grouper(key='Dt_Customer', freq='M')).size()

# Δημιουργία line plot για την παρακολούθηση των εγγραφών με την πάροδο του χρόνου
plt.figure(figsize=(12, 6))
plt.plot(registrations.index, registrations.values, marker='o', linestyle='-')
plt.title('Εγγραφές Πελατών ανά Μήνα')
plt.xlabel('Ημερομηνία')
plt.ylabel('Αριθμός Εγγραφών')
plt.grid(True)
plt.show()

# Μετατροπή της στήλης Dt_Customer σε datetime
df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d/%m/%Y')

# Ομαδοποίηση των εγγραφών ανά μήνα και υπολογισμός του μέσου όρου της Response
response_by_month = df.groupby(pd.Grouper(key='Dt_Customer', freq='M'))['Response'].mean()

# Σχεδίαση γραφήματος (line plot)
plt.figure(figsize=(12, 6))
plt.plot(response_by_month.index, response_by_month.values, marker='o', linestyle='-')
plt.title('Μέσος Όρος Response ανά Μήνα εγγραφής')
plt.xlabel('Ημερομηνία εγγραφής')
plt.ylabel('Μέσος Όρος Response')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()



# Mέγιστη ημερομηνία που υπάρχει στο dataset
reference_date = df['Dt_Customer'].max()

# Δημιουργία νέας στήλης που δείχνει πόσες μέρες έχουν περάσει από την εγγραφή ως την reference_date
df['days_from_registration'] = (reference_date - df['Dt_Customer']).dt.days

df['months_from_registration'] = df['days_from_registration'] / 30

print(df[['Dt_Customer', 'days_from_registration','months_from_registration']].head(10))

df.drop(['Dt_Customer',  'days_from_registration'], axis=1, inplace=True)

# print column types
df.dtypes

fig, ax = plt.subplots(figsize=(8, 5))
sns.countplot(data=df, x='Education', hue='Response')
ax.set_title('Response by education')
ax.set_xlabel('education')
ax.set_ylabel('count')
ax.legend(title='responsed', loc='upper right', labels=['No', 'Yes'])

fig, ax = plt.subplots(figsize=(8, 5))
sns.countplot(data=df, x='Kidhome', hue='Response')
ax.set_title('Response by Kidhome')
ax.set_xlabel('Kidhome')
ax.set_ylabel('count')
ax.legend(title='responsed', loc='upper right', labels=['No', 'Yes'])

fig, ax = plt.subplots(figsize=(8, 5))#parapano aftoi p den exoun efibous apantisi ne
sns.countplot(data=df, x='Teenhome', hue='Response')
ax.set_title('Response by Teenhome')
ax.set_xlabel('Teenhome')
ax.set_ylabel('count')
ax.legend(title='responsed', loc='upper right', labels=['No', 'Yes'])

fig, ax = plt.subplots(figsize=(10, 5))
sns.histplot(data=df, x='Income', bins=30, kde=True,hue='Response')
ax.set_title('Income Distribution')
ax.set_xlabel('Income')
ax.set_ylabel('Frequency')

max_idx = df['Income'].idxmax()
print("Διαγράφεται η γραμμή:", max_idx, "με income =", df.loc[max_idx, 'Income'])
df.drop(index=max_idx, inplace=True)

fig, ax = plt.subplots(figsize=(10, 5))
sns.histplot(data=df, x='Income', bins=30, kde=True,hue='Response')
ax.set_title('Income Distribution')
ax.set_xlabel('Income')
ax.set_ylabel('Frequency')

fig, ax = plt.subplots(figsize=(10, 5))
sns.histplot(data=df, x='Recency', bins=30, kde=True,hue='Response')
ax.set_title('Recency Distribution')
ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')

# Λίστα με τις μεταβλητές ενδιαφέροντος
features = [
    'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',
    'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',
    'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',
    'NumWebVisitsMonth'
]

# Ορισμός παραμέτρων για τα subplots (π.χ. 3 στήλες)
n_cols = 3
n_rows = (len(features) + n_cols - 1) // n_cols  # υπολογισμός γραμμών

fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
axes = axes.flatten()  # Για ευκολότερη επανάληψη

# Loop για κάθε feature
for i, feature in enumerate(features):
    sns.histplot(
        data=df, x=feature, bins=30, kde=True,
        hue='Response', ax=axes[i], palette=['blue', 'red']
    )
    axes[i].set_title(f'{feature} Distribution')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Frequency')

# Αφαίρεση επιπλέον subplots αν υπάρχουν
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# Λίστα με τις δυαδικές μεταβλητές
features_binary = [
    'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5',
    'AcceptedCmp1', 'AcceptedCmp2', 'Complain'
]

# Ορισμός παραμέτρων για τα subplots: π.χ. 3 στήλες
n_cols = 3
n_rows = (len(features_binary) + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
axes = axes.flatten()
# Loop για κάθε feature
for i, feature in enumerate(features_binary):
    sns.countplot(data=df, x=feature, hue='Response', ax=axes[i], palette=['red', 'blue'])
    axes[i].set_title(f'{feature} Distribution')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Count')

# Κατάργηση επιπλέον subplots (αν υπάρχουν)
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(10, 5))
sns.histplot(data=df, x='Marital_Status',  kde=True,hue='Response')
ax.set_title('Marital_Status Distribution')
ax.set_xlabel('Marital_Status')
ax.set_ylabel('Frequency')

fig, ax = plt.subplots(figsize=(10, 5))
sns.histplot(data=df, x='months_from_registration', bins=30, kde=True,hue='Response')
ax.set_title('months_from_registration Distribution')
ax.set_xlabel('months_from_registration')
ax.set_ylabel('Frequency')

print(df['months_from_registration'].describe())

skewness = df.select_dtypes(include=['int64', 'float64']).skew()
print(skewness.sort_values(ascending=False))

df.drop(['ID', 'Year_Birth'], axis=1, inplace=True)







# @title Split dataset
# Split features and target
X = df.drop('Response', axis=1)
y = df['Response']

# Split the data into training and test sets before scaling, encoding to avoid data leakage
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# @title Data Preprocessing and hyperparameter tuning

# @title Dataset v1

X_train_V_1 = X_train.copy()
X_test_V_1 = X_test.copy()

# Identify numerical and categorical features
num_features = X.select_dtypes(include=['int64', 'float64']).columns
cat_features = X.select_dtypes(include=['object']).columns
print(cat_features)

# Apply simple imputer with median strategy on numerical features | Income has outliers
si1 = SimpleImputer(strategy='median')
# fit (train) imputer on the training dataset
si1.fit(X_train_V_1[num_features])
# apply imputation on both training and test datasets
X_train_V_1[num_features] = si1.transform(X_train_V_1[num_features])
X_test_V_1[num_features] = si1.transform(X_test_V_1[num_features])

# Apply one hot encoding on categorical features
onehot_encoder = OneHotEncoder(
    use_cat_names=True,       # τα dummy columns θα έχουν ως suffix το όνομα της τιμής
    handle_unknown='value',   # οι άγνωστες κατηγορίες στο test θα γίνουν 0
    return_df=True
)

new_cols_train = onehot_encoder.fit_transform(X_train[['Marital_Status']])
# Concatenate the original DataFrame with the encoded DataFrame
X_train_V_1 = pd.concat([X_train_V_1, new_cols_train], axis=1)
# Drop the original categorical column if you no longer need it
X_train_V_1 = X_train_V_1.drop(columns=['Marital_Status'])

new_cols_test = onehot_encoder.transform(X_test[['Marital_Status']])
X_test_V_1 = pd.concat([X_test_V_1, new_cols_test], axis=1)
X_test_V_1 = X_test_V_1.drop(columns=['Marital_Status'])

edu_mapping = {
    'Basic':       1,
    '2n Cycle':    2,
    'Graduation':  3,
    'Master':      4,
    'PhD':         5,
}
# Apply ordinal encoding on categorical features
ordinal_encoder = OrdinalEncoder(return_df=True)

X_train_V_1[['Education']] = ordinal_encoder.fit_transform(X_train_V_1[['Education']])
X_test_V_1[['Education']] = ordinal_encoder.transform(X_test_V_1[['Education']])



# @title Dataset V2

X_train_V_2 = X_train.copy()
X_test_V_2 = X_test.copy()

# Identify numerical and categorical features
num_features = X.select_dtypes(include=['int64', 'float64']).columns
cat_features = X.select_dtypes(include=['object']).columns
print(cat_features)

# O mean itan kaliteros sinexizoume me mean
si1 = SimpleImputer(strategy='mean')
# fit (train) imputer on the training dataset
si1.fit(X_train_V_2[num_features])
# apply imputation on both training and test datasets
X_train_V_2[num_features] = si1.transform(X_train_V_2[num_features])
X_test_V_2[num_features] = si1.transform(X_test_V_2[num_features])

# Apply one hot encoding on categorical features
onehot_encoder = OneHotEncoder(
    use_cat_names=True,       # τα dummy columns θα έχουν ως suffix το όνομα της τιμής
    handle_unknown='value',   # οι άγνωστες κατηγορίες στο test θα γίνουν 0
    return_df=True
)

new_cols_train = onehot_encoder.fit_transform(X_train[['Marital_Status']])
# Concatenate the original DataFrame with the encoded DataFrame
X_train_V_2 = pd.concat([X_train_V_2, new_cols_train], axis=1)
# Drop the original categorical column if you no longer need it
X_train_V_2 = X_train_V_2.drop(columns=['Marital_Status'])

new_cols_test = onehot_encoder.transform(X_test[['Marital_Status']])
X_test_V_2 = pd.concat([X_test_V_2, new_cols_test], axis=1)
X_test_V_2 = X_test_V_2.drop(columns=['Marital_Status'])

edu_mapping = {
    'Basic':       1,
    '2n Cycle':    2,
    'Graduation':  3,
    'Master':      4,
    'PhD':         5,
}
# Apply ordinal encoding on categorical features
ordinal_encoder = OrdinalEncoder(return_df=True)

X_train_V_2[['Education']] = ordinal_encoder.fit_transform(X_train_V_2[['Education']])
X_test_V_2[['Education']] = ordinal_encoder.transform(X_test_V_2[['Education']])



# @title Dataset v3

X_train_V_3 = X_train.copy()
X_test_V_3 = X_test.copy()

# Identify numerical and categorical features
num_features = X.select_dtypes(include=['int64', 'float64']).columns
cat_features = X.select_dtypes(include=['object']).columns
print(cat_features)

# Apply simple imputer with median strategy on numerical features | Income has outliers
si1 = SimpleImputer(strategy='mean')
# fit (train) imputer on the training dataset
si1.fit(X_train_V_3[num_features])
# apply imputation on both training and test datasets
X_train_V_3[num_features] = si1.transform(X_train_V_3[num_features])
X_test_V_3[num_features] = si1.transform(X_test_V_3[num_features])

#EDUCATION ME ORDINAL ENCODING KALITERO
# Apply one hot encoding on categorical features
onehot_encoder = OneHotEncoder(
    use_cat_names=True,       # τα dummy columns θα έχουν ως suffix το όνομα της τιμής
    handle_unknown='value',   # οι άγνωστες κατηγορίες στο test θα γίνουν 0
    return_df=True
)

new_cols_train = onehot_encoder.fit_transform(X_train[['Marital_Status','Education']])
# Concatenate the original DataFrame with the encoded DataFrame
X_train_V_3 = pd.concat([X_train_V_3, new_cols_train], axis=1)
# Drop the original categorical column if you no longer need it
X_train_V_3 = X_train_V_3.drop(columns=['Marital_Status','Education'])

new_cols_test = onehot_encoder.transform(X_test[['Marital_Status','Education']])
X_test_V_3 = pd.concat([X_test_V_3, new_cols_test], axis=1)
X_test_V_3 = X_test_V_3.drop(columns=['Marital_Status','Education'])

# @title Dataset v3.1

X_train_V_3_1 = X_train.copy()
X_test_V_3_1 = X_test.copy()

# Identify numerical and categorical features
num_features = X.select_dtypes(include=['int64', 'float64']).columns
cat_features = X.select_dtypes(include=['object']).columns
print(cat_features)

# Apply simple imputer with median strategy on numerical features | Income has outliers
si1 = SimpleImputer(strategy='median')
# fit (train) imputer on the training dataset
si1.fit(X_train_V_3_1[num_features])
# apply imputation on both training and test datasets
X_train_V_3_1[num_features] = si1.transform(X_train_V_3_1[num_features])
X_test_V_3_1[num_features] = si1.transform(X_test_V_3_1[num_features])

#EDUCATION ME ORDINAL ENCODING KALITERO
# Apply one hot encoding on categorical features
onehot_encoder = OneHotEncoder(
    use_cat_names=True,       # τα dummy columns θα έχουν ως suffix το όνομα της τιμής
    handle_unknown='value',   # οι άγνωστες κατηγορίες στο test θα γίνουν 0
    return_df=True
)

new_cols_train = onehot_encoder.fit_transform(X_train[['Marital_Status','Education']])
# Concatenate the original DataFrame with the encoded DataFrame
X_train_V_3_1 = pd.concat([X_train_V_3_1, new_cols_train], axis=1)

X_train_V_3_1 = X_train_V_3_1.drop(columns=['Marital_Status','Education'])

new_cols_test = onehot_encoder.transform(X_test[['Marital_Status','Education']])
X_test_V_3_1 = pd.concat([X_test_V_3_1, new_cols_test], axis=1)
X_test_V_3_1 = X_test_V_3_1.drop(columns=['Marital_Status','Education'])

from feature_engine.selection import DropCorrelatedFeatures
corr_sel = DropCorrelatedFeatures(
    variables=None,      # None = όλες οι numeric στήλες
    method="pearson",
    threshold=0.80
)

# 3) fit στο train
corr_sel.fit(X_train_V_3_1)

# 4) transform σε train & test
X_train_V_3_1 = corr_sel.transform(X_train_V_3_1)
X_test_V_3_1  = corr_sel.transform(X_test_V_3_1)

dropped = corr_sel.features_to_drop_
kept    = [c for c in X_train_V_3_1.columns if c not in dropped]

print("Dropped columns:", dropped)
print("Kept columns:   ", kept)











# @title Dataset v4

X_train_V_4 = X_train_V_3_1.copy()
X_test_V_4 = X_test_V_3_1.copy()

#robust MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts Income
#johnson for unskewing

transformer = PowerTransformer(method='yeo-johnson')#standardize=True
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration','MntWines', 'MntFruits','MntSweetProducts', 'Income']


X_train_V_4[features_to_scale] = transformer.fit_transform(X_train_V_4[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_4[features_to_scale] = transformer.transform(X_test_V_4[features_to_scale])  # Transform test data
print(X_train_V_4[features_to_scale].describe())

import matplotlib.pyplot as plt

features = [
    'NumDealsPurchases', 'MntMeatProducts', 'MntFishProducts', 'MntGoldProds',
    'NumCatalogPurchases', 'NumWebPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',
    'Recency', 'months_from_registration', 'MntWines', 'MntFruits',
    'MntSweetProducts', 'Income'
]

df1 = X_train_V_4

for feat in features:
    plt.figure()
    plt.boxplot(df1[feat].dropna())
    plt.title(feat)
    plt.ylabel('Τιμές')
    plt.show()

for col in df1.columns:
    print(f"--- {col} ---")
    print(df1[col].describe())
    print()







# @title Dataset v4.1
#Να αποδειξω οτι με scaling o v3 ειναι καλυτερο απο v2

X_train_V_4_1 = X_train_V_2.copy()
X_test_V_4_1 = X_test_V_2.copy()

#robust MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts Income
#johnson for unskewing

transformer = PowerTransformer(method='yeo-johnson',standardize=True)#standardize=True
features_to_scale = ['NumDealsPurchases','Education'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration','MntWines', 'MntFruits','MntSweetProducts', 'Income']


X_train_V_4_1[features_to_scale] = transformer.fit_transform(X_train_V_4_1[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_4_1[features_to_scale] = transformer.transform(X_test_V_4_1[features_to_scale])  # Transform test data



# @title Dataset v5
 # ΔΕΝ ΧΡΗΣΗΜΟΠΟΙΗΘΗΚΕ BOXCOX ΛΩΓΟ ΜΗΔΕΝΙΚΩΝ ΤΙΜΩΝ
X_train_V_5 = X_train_V_3_1.copy()
X_test_V_5 = X_test_V_3_1.copy()

rs = RobustScaler()
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']
rs.fit(X_train_V_5[features_to_scale])
X_train_V_5[features_to_scale] = rs.transform(X_train_V_5[features_to_scale])
X_test_V_5[features_to_scale] = rs.transform(X_test_V_5[features_to_scale])





# @title Dataset v5.1

X_train_V_5_1 = X_train_V_2.copy()
X_test_V_5_1 = X_test_V_2.copy()

rs = RobustScaler()
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']
rs.fit(X_train_V_5_1[features_to_scale])
X_train_V_5_1[features_to_scale] = rs.transform(X_train_V_5_1[features_to_scale])
X_test_V_5_1[features_to_scale] = rs.transform(X_test_V_5_1[features_to_scale])



# @title Dataset v6
X_train_V_6 = X_train_V_3_1.copy()
X_test_V_6 = X_test_V_3_1.copy()

rs = RobustScaler()
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']
rs.fit(X_train_V_6[features_to_scale])
X_train_V_6[features_to_scale] = rs.transform(X_train_V_6[features_to_scale])
X_test_V_6[features_to_scale] = rs.transform(X_test_V_6[features_to_scale])

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']#education

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_6[features_to_scale])


X_train_V_6[features_to_scale] = scaler.transform(X_train_V_6[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_6[features_to_scale] = scaler.transform(X_test_V_6[features_to_scale])  # Transform test data

#scaler01 = MinMaxScaler(feature_range=(0,1))
#X_train_V_6[['Education']] = scaler01.fit_transform(X_train_V_6[['Education']])
#X_test_V_6[['Education']] = scaler01.transform(X_test_V_6[['Education']])

# @title Dataset v7
X_train_V_7 = X_train_V_3_1.copy()
X_test_V_7 = X_test_V_3_1.copy()

rs = RobustScaler()
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']
rs.fit(X_train_V_7[features_to_scale])
X_train_V_7[features_to_scale] = rs.transform(X_train_V_7[features_to_scale])
X_test_V_7[features_to_scale] = rs.transform(X_test_V_7[features_to_scale])

scaler01 = MinMaxScaler(feature_range=(0,1))
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']#,'Education']
X_train_V_7[features_to_scale] = scaler01.fit_transform(X_train_V_7[features_to_scale])
X_test_V_7[features_to_scale] = scaler01.transform(X_test_V_7[features_to_scale])









# @title Dataset v8
#COMBINATION
X_train_V_8 = X_train_V_3_1.copy()
X_test_V_8 = X_test_V_3_1.copy()

#johnson for unskewing
transformer = PowerTransformer(method='yeo-johnson',standardize=True)
features_to_scale = ['NumDealsPurchases','NumCatalogPurchases'
,'NumWebPurchases','Income','MntWines']


X_train_V_8[features_to_scale] = transformer.fit_transform(X_train_V_8[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_8[features_to_scale] = transformer.transform(X_test_V_8[features_to_scale])  # Transform test data

rs = RobustScaler()
features_to_scale = [ 'MntFruits', 'MntMeatProducts','MntFishProducts', 'MntSweetProducts','MntGoldProds']
rs.fit(X_train_V_8[features_to_scale])
X_train_V_8[features_to_scale] = rs.transform(X_train_V_8[features_to_scale])
X_test_V_8[features_to_scale] = rs.transform(X_test_V_8[features_to_scale])

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_8[features_to_scale])


X_train_V_8[features_to_scale] = scaler.transform(X_train_V_8[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_8[features_to_scale] = scaler.transform(X_test_V_8[features_to_scale])  # Transform test data

#scaler01 = MinMaxScaler(feature_range=(0,1))
#X_train_V_8[['Education']] = scaler01.fit_transform(X_train_V_8[['Education']])
#X_test_V_8[['Education']] = scaler01.transform(X_test_V_8[['Education']])



# @title Dataset v8.1

#COMBINATION
X_train_V_8_1 = X_train_V_3_1.copy()
X_test_V_8_1 = X_test_V_3_1.copy()

#johnson for unskewing
transformer = PowerTransformer(method='yeo-johnson')
features_to_scale = ['MntWines', 'MntFruits', 'MntMeatProducts',
'MntFishProducts', 'MntSweetProducts','MntGoldProds','NumDealsPurchases','NumCatalogPurchases'
,'NumWebPurchases']


X_train_V_8_1[features_to_scale] = transformer.fit_transform(X_train_V_8_1[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_8_1[features_to_scale] = transformer.transform(X_test_V_8_1[features_to_scale])  # Transform test data

rs = RobustScaler()
features_to_scale = ['NumDealsPurchases','NumCatalogPurchases'
,'NumWebPurchases','Income']
rs.fit(X_train_V_8_1[features_to_scale])
X_train_V_8_1[features_to_scale] = rs.transform(X_train_V_8_1[features_to_scale])
X_test_V_8_1[features_to_scale] = rs.transform(X_test_V_8_1[features_to_scale])

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_8_1[features_to_scale])


X_train_V_8_1[features_to_scale] = scaler.transform(X_train_V_8_1[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_8_1[features_to_scale] = scaler.transform(X_test_V_8_1[features_to_scale])  # Transform test data



# @title Dataset v9
#COMBINATION
X_train_V_9 = X_train_V_3_1.copy()
X_test_V_9 = X_test_V_3_1.copy()

#johnson for unskewing
transformer = PowerTransformer(method='yeo-johnson')
features_to_scale = ['NumDealsPurchases'#,'Education'
,'MntWines','NumCatalogPurchases'
,'NumWebPurchases','NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration', 'Income']


X_train_V_9[features_to_scale] = transformer.fit_transform(X_train_V_9[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_9[features_to_scale] = transformer.transform(X_test_V_9[features_to_scale])  # Transform test data

rs = RobustScaler()
features_to_scale = ['MntGoldProds', 'MntFruits', 'MntMeatProducts','MntFishProducts', 'MntSweetProducts']
rs.fit(X_train_V_9[features_to_scale])
X_train_V_9[features_to_scale] = rs.transform(X_train_V_9[features_to_scale])
X_test_V_9[features_to_scale] = rs.transform(X_test_V_9[features_to_scale])







# @title Dataset v10
X_train_V_10 = X_train_V_3_1.copy()
X_test_V_10 = X_test_V_3_1.copy()

#johnson for unskewing
transformer = PowerTransformer(method='yeo-johnson')
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','NumCatalogPurchases'
,'NumWebPurchases','Income','MntWines']


X_train_V_10[features_to_scale] = transformer.fit_transform(X_train_V_10[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_10[features_to_scale] = transformer.transform(X_test_V_10[features_to_scale])  # Transform test data

rs = RobustScaler()
features_to_scale = ['MntFruits', 'MntGoldProds','MntMeatProducts','MntFishProducts', 'MntSweetProducts']
rs.fit(X_train_V_10[features_to_scale])
X_train_V_10[features_to_scale] = rs.transform(X_train_V_10[features_to_scale])
X_test_V_10[features_to_scale] = rs.transform(X_test_V_10[features_to_scale])

scaler01 = MinMaxScaler(feature_range=(0,1))
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']#,'Education']
X_train_V_10[features_to_scale] = scaler01.fit_transform(X_train_V_10[features_to_scale])
X_test_V_10[features_to_scale] = scaler01.transform(X_test_V_10[features_to_scale])





# @title Dataset v11
#Με One-hot αποδιδει καλυτερα σε μετα το scaling
X_train_V_11 = X_train_V_3_1.copy()
X_test_V_11 = X_test_V_3_1.copy()

transformer = PowerTransformer(method='yeo-johnson')
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']


X_train_V_11[features_to_scale] = transformer.fit_transform(X_train_V_11[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_11[features_to_scale] = transformer.transform(X_test_V_11[features_to_scale])  # Transform test data

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']#education

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_11[features_to_scale])


X_train_V_11[features_to_scale] = scaler.transform(X_train_V_11[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_11[features_to_scale] = scaler.transform(X_test_V_11[features_to_scale])  # Transform test data

#scaler01 = MinMaxScaler(feature_range=(0,1))
#X_train_V_11[['Education']] = scaler01.fit_transform(X_train_V_11[['Education']])
#X_test_V_11[['Education']] = scaler01.transform(X_test_V_11[['Education']])





# @title Dataset v12

X_train_V_12 = X_train_V_3_1.copy()
X_test_V_12 = X_test_V_3_1.copy()

transformer = PowerTransformer(method='yeo-johnson')
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']


X_train_V_12[features_to_scale] = transformer.fit_transform(X_train_V_12[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_12[features_to_scale] = transformer.transform(X_test_V_12[features_to_scale])  # Transform test data

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']#education

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_12[features_to_scale])


X_train_V_12[features_to_scale] = scaler.transform(X_train_V_12[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_12[features_to_scale] = scaler.transform(X_test_V_12[features_to_scale])  # Transform test data









# @title Datase v13
X_train_V_13 = X_train_V_3_1.copy()
X_test_V_13 = X_test_V_3_1.copy()

transformer = PowerTransformer(method='yeo-johnson',standardize=True)###STANDARTIZE
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']


X_train_V_13[features_to_scale] = transformer.fit_transform(X_train_V_13[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_13[features_to_scale] = transformer.transform(X_test_V_13[features_to_scale])  # Transform test data

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration']#education

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_13[features_to_scale])


X_train_V_13[features_to_scale] = scaler.transform(X_train_V_13[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_13[features_to_scale] = scaler.transform(X_test_V_13[features_to_scale])  # Transform test data

import matplotlib.pyplot as plt



df3 = X_train_V_4
all_features = df3.columns

for feat in all_features:
    plt.figure()
    plt.boxplot(df3[feat].dropna())
    plt.title(feat)
    plt.ylabel('Τιμές')
    plt.show()



# @title Dataset v14

X_train_V_14 = X_train_V_3_1.copy()
X_test_V_14 = X_test_V_3_1.copy()

transformer = PowerTransformer(method='yeo-johnson',standardize=True)#Standardize
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts', 'Income']


X_train_V_14[features_to_scale] = transformer.fit_transform(X_train_V_14[features_to_scale])  # Fit and transform training data (Age and Fare columns only)
X_test_V_14[features_to_scale] = transformer.transform(X_test_V_14[features_to_scale])  # Transform test data

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency']#education

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_14[features_to_scale])


X_train_V_14[features_to_scale] = scaler.transform(X_train_V_14[features_to_scale])  # Fit and transform training data
X_test_V_14[features_to_scale] = scaler.transform(X_test_V_14[features_to_scale])  # Transform test data

scaler01 = MinMaxScaler(feature_range=(0,1))
X_train_V_14[['months_from_registration']] = scaler01.fit_transform(X_train_V_14[['months_from_registration']])
X_test_V_14[['months_from_registration']] = scaler01.transform(X_test_V_14[['months_from_registration']])









# @title Dataset v15

X_train_V_15 = X_train_V_3_1.copy()##### TO 3_1 CPPY INE TO KALO
X_test_V_15 = X_test_V_3_1.copy()



transformer = PowerTransformer(method='yeo-johnson')
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts','NumWebVisitsMonth']


X_train_V_15[features_to_scale] = transformer.fit_transform(X_train_V_15[features_to_scale])  # Fit and transform training data
X_test_V_15[features_to_scale] = transformer.transform(X_test_V_15[features_to_scale])  # Transform test data

#Robust
rs = RobustScaler()
features_to_scale = ['NumWebPurchases','NumWebVisitsMonth', 'Income']#['MntWines', 'MntFruits', 'MntMeatProducts','MntFishProducts', 'MntSweetProducts','NumWebVisitsMonth', 'Income']#,'NumDealsPurchases'#
rs.fit(X_train_V_15[features_to_scale])
X_train_V_15[features_to_scale] = rs.transform(X_train_V_15[features_to_scale])
X_test_V_15[features_to_scale] = rs.transform(X_test_V_15[features_to_scale])

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','Recency']#education#'NumWebVisitsMonth'#Income

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_15[features_to_scale])


X_train_V_15[features_to_scale] = scaler.transform(X_train_V_15[features_to_scale])  # Fit and transform training data
X_test_V_15[features_to_scale] = scaler.transform(X_test_V_15[features_to_scale])  # Transform test data

scaler01 = MinMaxScaler(feature_range=(0,1))
X_train_V_15[['months_from_registration']] = scaler01.fit_transform(X_train_V_15[['months_from_registration']])
X_test_V_15[['months_from_registration']] = scaler01.transform(X_test_V_15[['months_from_registration']])

print(X_train_V_15.describe())

import matplotlib.pyplot as plt

features = [
    'NumDealsPurchases', 'MntMeatProducts', 'MntFishProducts', 'MntGoldProds',
    'NumCatalogPurchases', 'NumWebPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',
    'Recency', 'months_from_registration', 'MntWines', 'MntFruits',
    'MntSweetProducts', 'Income'
]

df2 = X_train_V_15

for feat in features:
    plt.figure()
    plt.boxplot(df2[feat].dropna())
    plt.title(feat)
    plt.ylabel('Τιμές')
    plt.show()

for col in df2.columns:
    print(f"--- {col} ---")
    print(df2[col].describe())
    print()



# @title Dataset v16

X_train_V_16 = X_train_V_3_1.copy()##### TO 3_1 CPPY INE TO KALO
X_test_V_16 = X_test_V_3_1.copy()

transformer = PowerTransformer(method='yeo-johnson')
features_to_scale = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts']


X_train_V_16[features_to_scale] = transformer.fit_transform(X_train_V_16[features_to_scale])  # Fit and transform training data
X_test_V_16[features_to_scale] = transformer.transform(X_test_V_16[features_to_scale])  # Transform test data

#Robust
rs = RobustScaler()
features_to_scale = ['MntWines', 'MntFruits', 'MntMeatProducts','MntFishProducts', 'MntSweetProducts']#,'NumDealsPurchases'
rs.fit(X_train_V_16[features_to_scale])
X_train_V_16[features_to_scale] = rs.transform(X_train_V_16[features_to_scale])
X_test_V_16[features_to_scale] = rs.transform(X_test_V_16[features_to_scale])

# 4. Δημιουργία του scaler
scaler = StandardScaler()
features_to_scale = ['NumStorePurchases','NumWebVisitsMonth','Recency', 'Income','months_from_registration']#education

# 5. Fit μόνο στο train set
scaler.fit(X_train_V_16[features_to_scale])


X_train_V_16[features_to_scale] = scaler.transform(X_train_V_16[features_to_scale])  # Fit and transform training data
X_test_V_16[features_to_scale] = scaler.transform(X_test_V_16[features_to_scale])  # Transform test data



# @title Dataset v17

X_train_V_17 = X_train_V_15.copy()
X_test_V_17 = X_test_V_15.copy()



















featuresets = {
   'v1': X_train_V_1 ,
   'v2': X_train_V_2 ,
   'v3': X_train_V_3 ,
   'v4': X_train_V_4 ,
   'v4.1': X_train_V_4_1 ,
  'v5': X_train_V_5 ,
  'v5.1': X_train_V_5_1 ,
    'v6': X_train_V_6,
   'v7': X_train_V_7,
 'v8': X_train_V_8,
   'v8.1': X_train_V_8_1,
   'v9': X_train_V_9,
   'v10': X_train_V_10,
   'v11': X_train_V_11,
   'v12': X_train_V_12,
    'v13': X_train_V_13,
   'v14': X_train_V_14,
   'v15': X_train_V_15,
   'v16': X_train_V_16
}


# Define classifiers
classifiers = {
    "RandomForest": RandomForestClassifier(),
    "AdaBoost": AdaBoostClassifier(algorithm='SAMME'),
    "XGBoost": XGBClassifier(),
    "CatBoost": CatBoostClassifier(silent=True),
    "SVC": SVC(),
    "KNeighbors": KNeighborsClassifier(),
    "LogisticRegression": LogisticRegression(),
    "DecisionTree": DecisionTreeClassifier(),
    "GaussianNB": GaussianNB()
}

# Dictionary to store results
results = []

f1_score_weighted = make_scorer(f1_score, average='weighted')

# Loop over each featureset version
for featureset, X_data in featuresets.items():
    # Loop over each classifier
    print(featureset+":", end=' ... ')
    for clf_name, clf in classifiers.items():
        print(clf_name, end=' ... ')
        # Perform 10-fold cross-validation , no validation set need
        scores = cross_val_score(clf, X_data, y_train, cv=10, scoring=f1_score_weighted, n_jobs=-1)
        # Store the average score for this classifier and dataset version
        avg_score = scores.mean()
        results.append({
            'featureset': featureset,
            'classifier': clf_name,
            'score': avg_score
        })
    print()

# Convert results to DataFrame for easier analysis
results_df = pd.DataFrame(results)
results_df
# 1) Φιλτράρισμα
lr_results = results_df[results_df['classifier'] == 'LogisticRegression']

# 2) Εκτύπωση όλων των versions και των scores
print(lr_results)



# Best performing classifiers: SVC, AdaBoostClssifier
results_df.groupby(['classifier'])['score'].agg(['mean']).sort_values(by='mean', ascending=False)

# Best performing featureset versions: v1, v2
results_df.groupby(['featureset'])['score'].agg(['mean']).sort_values(by='mean', ascending=False)

# @title APROACH B

print(list(set(num_features) - set(features_to_scale)))
print(cat_features)
features_to_scale_V_4 = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','NumStorePurchases','NumWebVisitsMonth','Recency','months_from_registration','MntWines', 'MntFruits','MntSweetProducts', 'Income']
#features_to_scale_V_15_yj = ['NumDealsPurchases'
#,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
#,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts']
#features_to_scale_V_15_rs = ['MntWines', 'MntFruits', 'MntMeatProducts','MntFishProducts', 'MntSweetProducts']
#features_to_scale_V_15_ss = ['NumStorePurchases','NumWebVisitsMonth','Recency', 'Income']#education
#features_to_scale_V_15_mm = ['months_from_registration']

features_to_scale_yj = ['NumDealsPurchases'
,'MntMeatProducts','MntFishProducts','MntGoldProds','NumCatalogPurchases'
,'NumWebPurchases','MntWines', 'MntFruits', 'MntSweetProducts','NumWebVisitsMonth']


features_to_scale_rs = ['NumWebPurchases','NumWebVisitsMonth', 'Income']

features_to_scale_ss = ['NumStorePurchases','Recency']#education#'NumWebVisitsMonth'#Income

from catboost import CatBoostClassifier

num_pipeline0 = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
])

num_pipeline1 = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('unskewer', PowerTransformer(method='yeo-johnson'))#standardize=True
])

num_pipeline2 = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', RobustScaler())
])

# 1) Pipeline με StandardScaler
num_pipeline3 = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# 3) Pipeline για κατηγορηματικά (OneHot Encoding)
cat_pipeline = Pipeline([
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])


# preprocessing pipeline for creating featureset v4
preprocessor1 = ColumnTransformer(
    transformers=[
        ('num1', num_pipeline1, features_to_scale_V_4),                                 # numerical features to be scaled
        ('cat', cat_pipeline, cat_features)
    ],
    remainder='passthrough'
)
# preprocessing pipeline for creating featureset v15
preprocessor2 = ColumnTransformer(
    transformers=[
        ('num1', num_pipeline1, features_to_scale_yj),      # numerical features to be unskewed
        ('num2', num_pipeline2, features_to_scale_rs),      # numerical features to be unskewed
        ('num3', num_pipeline3, features_to_scale_ss),      # numerical features to be unskewed
        ('cat', cat_pipeline, cat_features)
    ],
    remainder='passthrough'
)
# IMPORTANT NOTICE: avoid using multiple consecutive column transformers since they alter column order

#X_transformed = preprocessor2.fit_transform(X_train)
#f = pd.DataFrame(X_transformed, columns = X_train.columns)
#print(f.head(3))

# Create 4 pipelines with different preprocessing steps
pipeline1 = Pipeline([
    ('preprocessor', preprocessor1),
    ('classifier', LogisticRegression())
])
pipeline2 = Pipeline([
    ('preprocessor', preprocessor1),
    ('classifier', CatBoostClassifier(verbose=0))
])
pipeline3 = Pipeline([
    ('preprocessor', preprocessor2),
    ('classifier', LogisticRegression())
])
pipeline4 = Pipeline([
    ('preprocessor', preprocessor2),
    ('classifier', CatBoostClassifier(verbose=0))
])
#pipeline5 = Pipeline([
#    ('preprocessor', preprocessor3),
#    ('classifier', CatBoostClassifier(verbose=0))
#])

# Define different pipelines with different classifiers and preprocessing steps
pipelines = {
    'LogisticRegression_v1_pipeline': pipeline1,
    'CatBoost_v1_pipeline': pipeline2,
    'LogisticRegression_v2_pipeline': pipeline3,
    'CatBoost_v2_pipeline': pipeline4
 #       'CatBoost_v3_pipeline': pipeline5
}

# Set up parameter grid for GridSearchCV to explore both pipelines
# 1) Grid για LogisticRegression

param_grid_logistic = [
    {
        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],
        'classifier__penalty': ['l2', 'l1'],               # τύποι regularization
        'classifier__solver': ['liblinear', 'saga'],       # solvers που υποστηρίζουν τις παραπάνω ποινές :contentReference[oaicite:0]{index=0}
        'classifier__class_weight': [None, 'balanced'],     # για imbalanced datasets
        'classifier__max_iter':  [200, 500, 1000]#####

    }
]



# 1) Επαυξημένο Grid για LogisticRegression

# 2) Επαυξημένο Grid για CatBoostClassifier
#param_grid_catboost = [
#    {
#      'classifier__iterations': [50, 100, 200, 500, 1000, 2000],
#        'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.3, 0.5, 1.0],
#        'classifier__depth': [3, 4, 6, 8, 10],
#        'classifier__l2_leaf_reg': [1, 3, 5, 7, 9, 11]
#    }
#]


# 2) Grid για CatBoostClassifier

param_grid_catboost = [
    {
        'classifier__iterations': [100, 200, 500],         # αριθμός δέντρων/boosting rounds
        'classifier__learning_rate': [0.01, 0.1, 0.5],     # βήμα ενημέρωσης των weights
        'classifier__depth': [4, 6, 8],                    # μέγιστο βάθος δέντρου
        'classifier__l2_leaf_reg': [3, 5, 7]               # L2 regularization στο φύλλο :contentReference[oaicite:1]{index=1}
    }
]

param_grids = {
    'LogisticRegression_v1_pipeline': param_grid_logistic,
    'CatBoost_v1_pipeline': param_grid_catboost,
    'LogisticRegression_v2_pipeline': param_grid_logistic,
    'CatBoost_v2_pipeline': param_grid_catboost,
    'CatBoost_v3_pipeline': param_grid_catboost

}

# Loop through each pipeline and perform GridSearchCV
best_estimators = {}
for pipeline_name, pipeline in pipelines.items():
    print(f"Running GridSearchCV for {pipeline_name}...")
    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grids[pipeline_name], cv=5, scoring=f1_score_weighted, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # Store the best estimator and results for each pipeline
    best_estimators[pipeline_name] = grid_search.best_estimator_
    print(f"Best Parameters for {pipeline_name}: {grid_search.best_params_}")
    print(f"Best Cross-Validated Score for {pipeline_name}: {grid_search.best_score_}")

    # Make predictions using the best estimator
    y_pred = grid_search.best_estimator_.predict(X_test)  # Use X_test for evaluation
    print(f"Performance on the test dataset: {f1_score(y_test, y_pred, average='weighted')}") # Evaluate on the test dataset
    print("")

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt
# Create the best model (using the best hyper-parameter values obtained frm the previous step)
#best_model = CatBoostClassifier(depth=4, iterations = 200,l2_leaf_reg = 3,learning_rate = 0.1)
best_model = LogisticRegression(C= 10, class_weight= None, max_iter= 500,penalty= 'l1', solver='saga')
# train the model on the dataset that achieves the highest score

sbs = SFS(best_model, # scikit-learn classifier
 k_features=(5, X_train_V_15.shape[1]-5), # termination criterion
 forward=False, # backward elimination
 floating=False,
 scoring='f1_weighted', # criterion function
 cv=5, # 10-fold cross validation
 n_jobs=-1)

sbs = sbs.fit(X_train_V_15, y_train)
print('\nSequential Backward Selection (k=5):')
print('Selected features:',sbs.k_feature_idx_)
print('Prediction (CV) score:',sbs.k_score_)
fig1 = plot_sfs(sbs.get_metric_dict(), kind='std_dev')
plt.ylim([0.8, 1])
plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()


X_train_V_15 = sbs.transform(X_train_V_15)
X_test_V_15  = sbs.transform(X_test_V_15)

best_model.fit(X_train_V_15, y_train)


# make prediction on the train dataset in order to check for overfitting
# (overfitting arises when high performance on training dataset and low performance on test dataset => model does not generalize
y_train_pred = best_model.predict(X_train_V_15)
print("F1 score on training dataset:", f1_score(y_train, y_train_pred, average='weighted'))

# make prediction on the test dataset
y_test_pred = best_model.predict(X_test_V_15)
print("F1 score on training dataset:", f1_score(y_test, y_test_pred, average='weighted'))

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt
import catboost
from catboost import CatBoostClassifier
# Create the best model (using the best hyper-parameter values obtained frm the previous step)
best_model = CatBoostClassifier(depth=4, iterations = 500,l2_leaf_reg = 7,learning_rate = 0.5,verbose=0)
#best_model = LogisticRegression(C= 1, class_weight= None, max_iter= 200,penalty= 'l2', solver='liblinear')
# train the model on the dataset that achieves the highest score

sbs = SFS(best_model, # scikit-learn classifier
 k_features=(5, X_train_V_4.shape[1]), # termination criterion
 forward=False, # backward elimination
 floating=False,
 scoring='f1_weighted', # criterion function
 cv=5, # 10-fold cross validation
 n_jobs=-1)
sbs = sbs.fit(X_train_V_4, y_train)
print('\nSequential Backward Selection (k=5):')
print('Selected features:',sbs.k_feature_idx_)# (0, 2, 8, 9, 12)
print('Prediction (CV) score:',sbs.k_score_) # 0.9607843137254901
fig1 = plot_sfs(sbs.get_metric_dict(), kind='std_dev')
plt.ylim([0.8, 1])
plt.title('Sequential Forward Selection (w. StdDev)')
plt.grid()
plt.show()




X_train_V_4 = sbs.transform(X_train_V_4)
X_test_V_4  = sbs.transform(X_test_V_4)

best_model.fit(X_train_V_4, y_train)







# make prediction on the train dataset in order to check for overfitting
# (overfitting arises when high performance on training dataset and low performance on test dataset => model does not generalize
y_train_pred = best_model.predict(X_train_V_4)
print("F1 score on training dataset:", f1_score(y_train, y_train_pred, average='weighted'))

# make prediction on the test dataset
y_test_pred = best_model.predict(X_test_V_4)
print("F1 score on training dataset:", f1_score(y_test, y_test_pred, average='weighted'))





